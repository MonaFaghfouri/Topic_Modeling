{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonaFaghfouri/Topic_Modeling/blob/main/Topic_Modeling_Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install required libraries\n",
        "!pip install gensim sentence-transformers umap-learn hdbscan pandas numpy matplotlib scikit-learn pyLDAvis"
      ],
      "metadata": {
        "id": "vFY1M0KjDPRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap.umap_ as umap\n",
        "import hdbscan\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "class AdvancedTopicModel:\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "        self.vectorizer = CountVectorizer(stop_words=None, max_features=10000)\n",
        "\n",
        "    def upload_file(self):\n",
        "        print(\"üì§ Please upload an Excel or CSV file containing texts:\")\n",
        "        uploaded = files.upload()\n",
        "        file_name = next(iter(uploaded))\n",
        "\n",
        "        if file_name.endswith('.xlsx'):\n",
        "            df = pd.read_excel(file_name)\n",
        "        elif file_name.endswith('.csv'):\n",
        "            df = pd.read_csv(file_name)\n",
        "        else:\n",
        "            raise ValueError(\"File format must be .xlsx or .csv\")\n",
        "\n",
        "        texts = df.iloc[:, 1].astype(str).tolist()\n",
        "        return [text for text in texts if len(text.split()) > 1]\n",
        "\n",
        "    def preprocess(self, texts):\n",
        "        cleaned = []\n",
        "        for text in texts:\n",
        "            tokens = text.replace('\\n', ' ').replace('\\r', '').strip().split(',')\n",
        "            unique_tokens = list(dict.fromkeys([t.strip() for t in tokens if t.strip() != '']))\n",
        "            cleaned.append(\" \".join(unique_tokens))\n",
        "        return cleaned\n",
        "\n",
        "    def extract_topics(self, texts):\n",
        "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        best_result = {\n",
        "            \"score\": -1,\n",
        "            \"topics\": None,\n",
        "            \"clusters\": None,\n",
        "            \"texts\": None,\n",
        "            \"embeddings\": None,\n",
        "            \"silhouette\": None,\n",
        "            \"db_index\": None,\n",
        "            \"n_clusters\": None,\n",
        "            \"params\": None\n",
        "        }\n",
        "\n",
        "        print(\"\\nüîç Searching for best clustering parameters...\")\n",
        "\n",
        "        for n_neighbors in [10, 15]:\n",
        "            for min_cluster_size in [3, 5, 8]:\n",
        "                umap_embeddings = umap.UMAP(\n",
        "                    n_neighbors=n_neighbors,\n",
        "                    n_components=5,\n",
        "                    metric='cosine',\n",
        "                    random_state=42\n",
        "                ).fit_transform(embeddings)\n",
        "\n",
        "                clusterer = hdbscan.HDBSCAN(\n",
        "                    min_cluster_size=min_cluster_size,\n",
        "                    min_samples=2,\n",
        "                    metric='euclidean',\n",
        "                    cluster_selection_method='eom'\n",
        "                )\n",
        "                clusters = clusterer.fit_predict(umap_embeddings)\n",
        "\n",
        "                valid_indices = [i for i, c in enumerate(clusters) if c != -1]\n",
        "                if len(valid_indices) < 10:\n",
        "                    continue\n",
        "\n",
        "                filtered_embeddings = umap_embeddings[valid_indices]\n",
        "                filtered_clusters = clusters[valid_indices]\n",
        "                filtered_texts = [texts[i] for i in valid_indices]\n",
        "\n",
        "                cluster_counts = pd.Series(filtered_clusters).value_counts()\n",
        "                valid_cluster_ids = cluster_counts[cluster_counts >= 5].index\n",
        "\n",
        "                final_texts = []\n",
        "                final_clusters = []\n",
        "                final_embeddings = []\n",
        "\n",
        "                for i, (text, cluster, emb) in enumerate(zip(filtered_texts, filtered_clusters, filtered_embeddings)):\n",
        "                    if cluster in valid_cluster_ids:\n",
        "                        final_texts.append(text)\n",
        "                        final_clusters.append(cluster)\n",
        "                        final_embeddings.append(emb)\n",
        "\n",
        "                final_clusters = np.array(final_clusters)\n",
        "                final_embeddings = np.array(final_embeddings)\n",
        "\n",
        "                if len(set(final_clusters)) <= 1:\n",
        "                    continue\n",
        "\n",
        "                silhouette = silhouette_score(final_embeddings, final_clusters)\n",
        "                db_index = davies_bouldin_score(final_embeddings, final_clusters)\n",
        "                score = silhouette - db_index\n",
        "\n",
        "                if score > best_result[\"score\"]:\n",
        "                    vectorizer = self.vectorizer.fit(final_texts)\n",
        "                    vocab = vectorizer.get_feature_names_out()\n",
        "                    word_counts = vectorizer.transform(final_texts)\n",
        "\n",
        "                    topics = {}\n",
        "                    for cluster_id in set(final_clusters):\n",
        "                        indices = [i for i, c in enumerate(final_clusters) if c == cluster_id]\n",
        "                        cluster_texts = [final_texts[i] for i in indices]\n",
        "                        word_freq = word_counts[indices].sum(axis=0).A1\n",
        "                        top_words_idx = word_freq.argsort()[::-1][:10]\n",
        "                        top_words = [vocab[i] for i in top_words_idx]\n",
        "\n",
        "                        topics[cluster_id] = {\n",
        "                            'words': top_words,\n",
        "                            'sample_texts': cluster_texts[:3]\n",
        "                        }\n",
        "\n",
        "                    best_result.update({\n",
        "                        \"score\": score,\n",
        "                        \"topics\": topics,\n",
        "                        \"clusters\": final_clusters,\n",
        "                        \"texts\": final_texts,\n",
        "                        \"embeddings\": final_embeddings,\n",
        "                        \"silhouette\": silhouette,\n",
        "                        \"db_index\": db_index,\n",
        "                        \"n_clusters\": len(set(final_clusters)),\n",
        "                        \"params\": (n_neighbors, min_cluster_size)\n",
        "                    })\n",
        "\n",
        "        if best_result[\"topics\"] is None:\n",
        "            print(\"‚ùå No high-quality clusters were found.\")\n",
        "            return {}\n",
        "\n",
        "        print(\"\\n‚úÖ Best result found with parameters:\")\n",
        "        print(\"üîπ n_neighbors =\", best_result[\"params\"][0])\n",
        "        print(\"üîπ min_cluster_size =\", best_result[\"params\"][1])\n",
        "        print(\"üìà Silhouette Score:\", round(best_result[\"silhouette\"], 3))\n",
        "        print(\"üìâ Davies-Bouldin Index:\", round(best_result[\"db_index\"], 3))\n",
        "        print(\"üß† Final number of clusters:\", best_result[\"n_clusters\"])\n",
        "\n",
        "        self.clusters = best_result[\"clusters\"]\n",
        "        self.texts = best_result[\"texts\"]\n",
        "        self.topics = best_result[\"topics\"]\n",
        "        self.umap_embeddings = best_result[\"embeddings\"]\n",
        "\n",
        "        return best_result[\"topics\"]\n",
        "\n",
        "    def save_to_excel(self):\n",
        "        data = []\n",
        "        for i, (text, cluster) in enumerate(zip(self.texts, self.clusters)):\n",
        "            data.append({\n",
        "                'Index': i + 1,\n",
        "                'Text': text,\n",
        "                'Cluster': cluster\n",
        "            })\n",
        "\n",
        "        df_out = pd.DataFrame(data)\n",
        "        file_name = \"Optimal_Topic_Clusters.xlsx\"\n",
        "        df_out.to_excel(file_name, index=False)\n",
        "        print(f\"\\nüìÅ Output file saved as '{file_name}'.\")\n",
        "        files.download(file_name)\n",
        "\n",
        "    def run(self):\n",
        "        texts = self.upload_file()\n",
        "        texts = self.preprocess(texts)\n",
        "\n",
        "        print(\"\\nüîÆ Analyzing topics...\")\n",
        "        self.extract_topics(texts)\n",
        "\n",
        "        self.save_to_excel()\n",
        "\n",
        "# Run the model\n",
        "if __name__ == \"__main__\":\n",
        "    model = AdvancedTopicModel()\n",
        "    model.run()\n"
      ],
      "metadata": {
        "id": "tN3eiIEwCs-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming model.topics is a dictionary like:\n",
        "# {cluster_id: {'words': [word1, word2, ...]}}\n",
        "\n",
        "# Prepare data for output\n",
        "data = []\n",
        "for cluster_id, topic in model.topics.items():\n",
        "    words = \", \".join(topic['words'])\n",
        "    print(f\"Cluster {cluster_id}: {words}\")  # Print to console\n",
        "    data.append({'Cluster': cluster_id, 'Keywords': words})\n",
        "\n",
        "# Create Excel file\n",
        "df = pd.DataFrame(data)\n",
        "file_name = 'Cluster_Keywords_Output.xlsx'\n",
        "df.to_excel(file_name, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Output file named '{file_name}' was saved successfully.\")\n"
      ],
      "metadata": {
        "id": "wSLLBrdRC7gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print topics of each cluster\n",
        "for cluster_id, topic in model.topics.items():\n",
        "    words = \", \".join(topic['words'])\n",
        "    print(f\"Cluster {cluster_id}: {words}\")\n"
      ],
      "metadata": {
        "id": "P1eX31EvDHor"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}