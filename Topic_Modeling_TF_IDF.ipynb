{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonaFaghfouri/Topic_Modeling/blob/main/Topic_Modeling_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall numpy pandas gensim openpyxl"
      ],
      "metadata": {
        "id": "Uz5VY24aPEuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Upload Excel file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 2. Read the data\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "df = pd.read_excel(next(iter(uploaded)))\n",
        "\n",
        "# 3. Evaluate token list from string format\n",
        "texts = df.iloc[:, 1].astype(str).apply(ast.literal_eval).tolist()\n",
        "\n",
        "# 4. Remove empty documents\n",
        "texts = [doc for doc in texts if len(doc) > 0]\n",
        "\n",
        "# 5. Create dictionary and corpus\n",
        "from gensim import corpora, models\n",
        "\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "print(f\"Unique tokens before filtering: {len(dictionary)}\")\n",
        "\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.9)\n",
        "print(f\"Unique tokens after filtering: {len(dictionary)}\")\n",
        "\n",
        "if len(dictionary) == 0:\n",
        "    raise ValueError(\"Dictionary is empty after filtering. Adjust thresholds.\")\n",
        "\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# 6. Apply TF-IDF transformation\n",
        "tfidf_model = models.TfidfModel(bow_corpus)\n",
        "tfidf_corpus = tfidf_model[bow_corpus]\n",
        "\n",
        "# 7. Optimize LDA with TF-IDF corpus\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def optimize_lda_tfidf(dictionary, tfidf_corpus, texts, topic_range=(5, 16), max_words=20):\n",
        "    best_model = None\n",
        "    best_topic_num = 0\n",
        "    best_num_words = 0\n",
        "    best_coherence = float('-inf')\n",
        "    best_perplexity = float('inf')\n",
        "    best_combination = None\n",
        "    results = []\n",
        "\n",
        "    for num_topics in range(*topic_range):\n",
        "        print(f\"\\n🔄 Testing {num_topics} topics...\")\n",
        "        model = LdaModel(\n",
        "            corpus=tfidf_corpus,\n",
        "            id2word=dictionary,\n",
        "            num_topics=num_topics,\n",
        "            passes=30,\n",
        "            iterations=600,\n",
        "            random_state=42,\n",
        "            alpha='auto',\n",
        "            eta='auto',\n",
        "            eval_every=None\n",
        "        )\n",
        "\n",
        "        for num_words in range(5, max_words + 1):\n",
        "            topics = model.show_topics(num_topics=-1, num_words=num_words, formatted=False)\n",
        "            topic_word_lists = [[word for word, _ in topic[1]] for topic in topics]\n",
        "\n",
        "            cm = CoherenceModel(\n",
        "                topics=topic_word_lists,\n",
        "                texts=texts,\n",
        "                dictionary=dictionary,\n",
        "                coherence='c_v'\n",
        "            )\n",
        "            coherence = cm.get_coherence()\n",
        "\n",
        "            # Synthetic corpus for perplexity\n",
        "            synthetic_corpus = []\n",
        "            for topic in topics:\n",
        "                words = [word for word, _ in topic[1]]\n",
        "                bow = dictionary.doc2bow(words)\n",
        "                tfidf_bow = tfidf_model[bow]\n",
        "                synthetic_corpus.append(tfidf_bow)\n",
        "            perplexity = model.log_perplexity(synthetic_corpus)\n",
        "\n",
        "            results.append((num_topics, num_words, coherence, perplexity))\n",
        "            print(f\"Topics: {num_topics} | Words: {num_words} → Coherence: {coherence:.4f} | Perplexity: {perplexity:.4f}\")\n",
        "\n",
        "            if coherence > best_coherence or (coherence == best_coherence and perplexity < best_perplexity):\n",
        "                best_model = model\n",
        "                best_topic_num = num_topics\n",
        "                best_num_words = num_words\n",
        "                best_coherence = coherence\n",
        "                best_perplexity = perplexity\n",
        "                best_combination = (num_topics, num_words)\n",
        "\n",
        "    return best_model, best_combination, best_coherence, best_perplexity, results\n",
        "\n",
        "# 8. Run optimization\n",
        "print(\"\\n🔍 Finding best topic/word configuration (TF-IDF)...\")\n",
        "best_model, best_combo, best_coh, best_perp, all_results = optimize_lda_tfidf(\n",
        "    dictionary, tfidf_corpus, texts, topic_range=(5, 16), max_words=20\n",
        ")\n",
        "\n",
        "# 9. Print best configuration\n",
        "print(f\"\\n✅ Best Model: {best_combo[0]} Topics | {best_combo[1]} Words per Topic\")\n",
        "print(f\"   Coherence: {best_coh:.4f} | Perplexity: {best_perp:.4f}\")\n",
        "\n",
        "# 10. Show best topics\n",
        "print(\"\\n🧠 Best Topics (TF-IDF):\")\n",
        "topics = best_model.show_topics(num_topics=-1, num_words=best_combo[1], formatted=False)\n",
        "for topic_id, words in topics:\n",
        "    word_list = \", \".join(word for word, _ in words)\n",
        "    print(f\"Topic {topic_id}: {word_list}\")\n"
      ],
      "metadata": {
        "id": "wvgIEIEGARa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Upload files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 2. Libraries\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import CoherenceModel, LdaModel\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 3. Identify input files\n",
        "file_names = list(uploaded.keys())\n",
        "sample_words_file = [f for f in file_names if 'word' in f.lower()][0]\n",
        "text_file = [f for f in file_names if f != sample_words_file][0]\n",
        "\n",
        "# 4. Word cleaning function\n",
        "def clean_word(word):\n",
        "    return re.sub(r'[^\\w\\s]', '', word.strip().lower())\n",
        "\n",
        "# 5. Read list of economic words\n",
        "df_words = pd.read_excel(sample_words_file)\n",
        "economic_words = df_words.iloc[:, 0].dropna().astype(str).apply(clean_word).tolist()\n",
        "economic_words = set(economic_words)\n",
        "\n",
        "# 6. Process text file\n",
        "df_texts = pd.read_excel(text_file)\n",
        "texts_raw = df_texts.iloc[:, 1]\n",
        "\n",
        "cleaned_docs = []\n",
        "for val in texts_raw:\n",
        "    try:\n",
        "        parsed = ast.literal_eval(str(val))\n",
        "        if isinstance(parsed, list):\n",
        "            cleaned = [clean_word(w) for w in parsed if clean_word(w) in economic_words]\n",
        "            if cleaned:\n",
        "                cleaned_docs.append(cleaned)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(f\"📊 Number of documents containing economic words: {len(cleaned_docs)}\")\n",
        "if not cleaned_docs:\n",
        "    raise ValueError(\"❌ No documents contain economic words.\")\n",
        "\n",
        "# 7. Create dictionary and corpus\n",
        "dictionary = corpora.Dictionary(cleaned_docs)\n",
        "corpus = [dictionary.doc2bow(text) for text in cleaned_docs]\n",
        "\n",
        "# 8. Find best number of topics based on coherence score\n",
        "print(\"🔍 Finding best number of topics based on coherence...\")\n",
        "best_model = None\n",
        "best_coherence = -1\n",
        "best_topics = 0\n",
        "\n",
        "for num_topics in range(5, 21):  # Evaluate topics from 5 to 20\n",
        "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)\n",
        "    coherence_model = CoherenceModel(model=lda, texts=cleaned_docs, dictionary=dictionary, coherence='c_v')\n",
        "    coherence = coherence_model.get_coherence()\n",
        "\n",
        "    print(f\"→ Topics: {num_topics}, Coherence: {coherence:.4f}\")\n",
        "\n",
        "    if coherence > best_coherence:\n",
        "        best_coherence = coherence\n",
        "        best_model = lda\n",
        "        best_topics = num_topics\n",
        "\n",
        "# 9. Display final model output\n",
        "lda_model = best_model\n",
        "perplexity_score = lda_model.log_perplexity(corpus)\n",
        "\n",
        "print(f\"\\n✅ Best model found with {best_topics} topics.\")\n",
        "print(f\"📈 Coherence Score (c_v): {best_coherence:.4f}\")\n",
        "print(f\"📉 Perplexity: {perplexity_score:.4f}\")\n",
        "\n",
        "# 10. Print final topics\n",
        "num_words = 15\n",
        "print(f\"\\n🧠 Final Topics (Top {num_words} words per topic):\")\n",
        "for idx, topic in lda_model.print_topics(num_topics=best_topics, num_words=num_words):\n",
        "    words = re.findall(r'\"(.*?)\"', topic)\n",
        "    if len(words) >= 2:\n",
        "        print(f\"Topic {idx}: {', '.join(words)}\")\n"
      ],
      "metadata": {
        "id": "fSFlbaSfxUVR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}