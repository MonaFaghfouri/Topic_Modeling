{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonaFaghfouri/Topic_Modeling/blob/main/Topic_Modeling_Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvW79kdagsdz"
      },
      "outputs": [],
      "source": [
        "!pip install pandas openpyxl gensim scikit-learn matplotlib\n",
        "!pip install --upgrade --force-reinstall numpy pandas gensim openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjzhgZEkeL2D"
      },
      "outputs": [],
      "source": [
        "# üìÅ 1. Upload Excel File\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# üìä 2. Read and Prepare Data\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "df = pd.read_excel(next(iter(uploaded)))\n",
        "df['lemmatized_tweet'] = df['Text'].astype(str).apply(lambda x: ' '.join(ast.literal_eval(x)))\n",
        "\n",
        "# üß† 3. Import Required Libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "import numpy as np\n",
        "\n",
        "# üîç 4. TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
        "tfidf_matrix = vectorizer.fit_transform(df['lemmatized_tweet'])\n",
        "\n",
        "# üîÅ 5. Grid Search to Optimize DBSCAN Parameters\n",
        "best_score = -1\n",
        "best_eps = None\n",
        "best_min_samples = None\n",
        "\n",
        "print(\"üîç Searching for the best amount of eps Ÿà min_samples ...\")\n",
        "\n",
        "for eps in np.arange(0.1, 1.2, 0.1):\n",
        "    for min_samples in range(3, 15):\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(tfidf_matrix)\n",
        "\n",
        "        mask = labels != -1\n",
        "        if len(set(labels[mask])) < 2:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            score = silhouette_score(tfidf_matrix[mask], labels[mask])\n",
        "            print(f\"eps={eps:.1f}, min_samples={min_samples}, silhouette={score:.4f}\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_eps = eps\n",
        "                best_min_samples = min_samples\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "# ‚úÖ 6. Final Clustering with Best Parameters\n",
        "print(\"\\n‚úÖ The best combination was found:\")\n",
        "print(f\"Best eps = {best_eps}, Best min_samples = {best_min_samples}, Silhouette Score = {best_score:.4f}\")\n",
        "\n",
        "dbscan_final = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
        "final_labels = dbscan_final.fit_predict(tfidf_matrix)\n",
        "\n",
        "# üßº 7. Filter Non-Noise Data\n",
        "mask = final_labels != -1\n",
        "filtered_labels = final_labels[mask]\n",
        "tfidf_dense = tfidf_matrix[mask].toarray()\n",
        "\n",
        "# üìè 8. Final Evaluation\n",
        "if len(set(filtered_labels)) > 1:\n",
        "    silhouette_final = silhouette_score(tfidf_dense, filtered_labels)\n",
        "    db_index = davies_bouldin_score(tfidf_dense, filtered_labels)\n",
        "    print(f\"\\nüìà Final Silhouette Score: {silhouette_final:.4f}\")\n",
        "    print(f\"üìâ Davies-Bouldin Index: {db_index:.4f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Clustering was not successful (only noise or a single cluster).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo5JWyEcpJpf"
      },
      "outputs": [],
      "source": [
        "# üìÅ 1. Upload Excel File\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# üìä 2. Read and Prepare Data\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "df = pd.read_excel(next(iter(uploaded)))\n",
        "df['lemmatized_tweet'] = df['Text'].astype(str).apply(lambda x: ' '.join(ast.literal_eval(x)))\n",
        "\n",
        "# üß† 3. Import Libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# üîç 4. TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
        "tfidf_matrix = vectorizer.fit_transform(df['lemmatized_tweet'])\n",
        "\n",
        "# üß© 5. DBSCAN Clustering\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=3)\n",
        "labels = dbscan.fit_predict(tfidf_matrix)\n",
        "\n",
        "# üßº 6. Filter Non-Noise Data\n",
        "mask = labels != -1\n",
        "filtered_df = df[mask].copy()\n",
        "filtered_df['cluster'] = labels[mask]\n",
        "\n",
        "# üß† 7. Extract Top Words Per Cluster\n",
        "cluster_tokens = {}\n",
        "for cluster_id in sorted(filtered_df['cluster'].unique()):\n",
        "    cluster_texts = filtered_df[filtered_df['cluster'] == cluster_id]['lemmatized_tweet']\n",
        "    all_tokens = ' '.join(cluster_texts).split()\n",
        "    common_words = [word for word, _ in Counter(all_tokens).most_common(5)]\n",
        "    cluster_tokens[cluster_id] = common_words\n",
        "\n",
        "# üñ®Ô∏è 8. Print Topics\n",
        "print(\"\\nüß† Best Topics (with eps=0.3):\")\n",
        "for cluster_id, words in cluster_tokens.items():\n",
        "    word_str = 'ÿå '.join(words)\n",
        "    print(f\"Topic {cluster_id}: {word_str}\")\n",
        "\n",
        "# üìâ 9. DBSCAN Cluster Visualization\n",
        "print(\"\\nüìä Generating Cluster Visualization...\")\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_data = pca.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.scatter(reduced_data[mask, 0], reduced_data[mask, 1], c=labels[mask], cmap='viridis', s=15)\n",
        "plt.title(\"DBSCAN Cluster Visualization\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.grid(True)\n",
        "plt.savefig(\"dbscan_clusters.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# üìä 10. Cluster Size Distribution (Readable)\n",
        "plt.figure(figsize=(14, 6))  # ÿ®ÿ≤ÿ±⁄Ø‚Äåÿ™ÿ± ÿ¥ÿØŸÜ ÿ™ÿµŸà€åÿ±\n",
        "cluster_counts = pd.Series(labels[mask]).value_counts().sort_index()\n",
        "sns.barplot(x=cluster_counts.index, y=cluster_counts.values, width=0.7)\n",
        "\n",
        "plt.xlabel(\"Cluster\", fontsize=12)\n",
        "plt.ylabel(\"Number of Data Points\", fontsize=12)\n",
        "plt.title(\"Cluster Size Distribution\", fontsize=14)\n",
        "plt.xticks(rotation=90, fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y')\n",
        "plt.savefig(\"cluster_sizes_readable.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# üìè Silhouette Score\n",
        "if len(set(labels[mask])) > 1:\n",
        "    silhouette = silhouette_score(tfidf_matrix[mask], labels[mask])\n",
        "    print(f\"\\nüìà Silhouette Score (excluding noise): {silhouette:.4f}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Not enough clusters to compute Silhouette Score.\")\n",
        "\n",
        "# üì• 11. Download Saved Figures\n",
        "files.download(\"dbscan_clusters.png\")\n",
        "files.download(\"cluster_sizes_readable.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIdX8K95uEDc"
      },
      "outputs": [],
      "source": [
        "# 1. Upload files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 2. Libraries\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 3. Identify filtered word list file and text file\n",
        "file_names = list(uploaded.keys())\n",
        "sample_words_file = [f for f in file_names if 'word' in f.lower()][0]\n",
        "text_file = [f for f in file_names if f != sample_words_file][0]\n",
        "\n",
        "# 4. Word cleaning function\n",
        "def clean_word(word):\n",
        "    return re.sub(r'[^\\w\\s]', '', word.strip().lower())\n",
        "\n",
        "# 5. Read and clean filtered words\n",
        "df_words = pd.read_excel(sample_words_file)\n",
        "economic_words = df_words.iloc[:, 0].dropna().astype(str).apply(clean_word).tolist()\n",
        "economic_words = set(economic_words)\n",
        "\n",
        "# 6. Read and process text file\n",
        "df_texts = pd.read_excel(text_file)\n",
        "texts_raw = df_texts.iloc[:, 1]\n",
        "\n",
        "texts = []\n",
        "for val in texts_raw:\n",
        "    try:\n",
        "        parsed = ast.literal_eval(str(val))\n",
        "        if isinstance(parsed, list):\n",
        "            cleaned = [clean_word(w) for w in parsed if clean_word(w) in economic_words]\n",
        "            if cleaned:\n",
        "                texts.append(cleaned)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(f\"üìä Number of documents containing filtered words: {len(texts)}\")\n",
        "if not texts:\n",
        "    raise ValueError(\"‚ùå No documents contain the filtered words.\")\n",
        "\n",
        "# 7. Train Word2Vec model\n",
        "print(\"üß† Training Word2Vec model...\")\n",
        "w2v_model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=2, workers=4, sg=1, seed=42)\n",
        "word_vectors = w2v_model.wv\n",
        "\n",
        "print(f\"üî¢ Vocabulary size: {len(word_vectors.index_to_key)}\")\n",
        "\n",
        "# 8. Extract word vectors for clustering\n",
        "valid_words = list(word_vectors.index_to_key)\n",
        "X = np.array([word_vectors[word] for word in valid_words])\n",
        "\n",
        "# 9. Cluster word vectors\n",
        "n_clusters = 10  # You can adjust this\n",
        "print(f\"üîç Clustering words into {n_clusters} topics...\")\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# 10. Show top words in each cluster\n",
        "print(\"\\nüß† Word clusters based on Word2Vec similarity:\")\n",
        "clustered_words = {i: [] for i in range(n_clusters)}\n",
        "for word, label in zip(valid_words, labels):\n",
        "    clustered_words[label].append(word)\n",
        "\n",
        "for cluster_id, words in clustered_words.items():\n",
        "    print(f\"Cluster {cluster_id}: {', '.join(words[:15])}\")  # show top 15 words per cluster\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}